{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training VAE on USDA SR legacy food dataset\n\n**Author:** [Yoga Harshitha Duddukuri](https://www.linkedin.com/in/dyogaharshitha)<br>\n\n**Description:** USDA SR legacy food dataset was cleaned and processed. Encoder in VAE reduces the dimension to 50 , which was originally 100, decoder retrives the data with MAE of 0.07","metadata":{"id":"dyHETBSyjORe"}},{"cell_type":"markdown","source":"## Import modules","metadata":{"id":"AI_h0sNgjORl"}},{"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow import keras \nfrom keras import layers\nimport tensorflow_probability as tfp\n\nimport pandas as pd\nimport numpy as np","metadata":{"id":"N-tBlPrCjORm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport sys\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{"id":"bsa4YmyKjORo"}},{"cell_type":"code","source":"#data\nbatch_size = 14164\n\n\n# optimization\nbatch_size =  14164      \nlearning_rate = 0.0005 \nweight_decay = 1e-4 \nopt = tf.keras.optimizers.Adam(0.0005,beta_1=0.8,beta_2=0.88,epsilon=1e-5)\n\n","metadata":{"id":"MFJIxmMjjORo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pipeline\n\nWe use the\n[USDA SR Legacy dataset](https://fdc.nal.usda.gov/download-datasets.html)\n for encoding the nutrition data of food items. USDA food database has nutrition information of various food items, which can be handy while generating a meal plan to meet the nutritional requirements. Redundant and irrelevent columns are removed and Data set is cleaned. \n\n Below code uses pandas dataframe to preprocess the data. ","metadata":{"id":"qdn-Qf4NjORo"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\"food data\"\nusda_food = pd.read_csv(\"/kaggle/input/usda-sr-legacy-and-foundation-nutrition-for-use/usda_sr_all_foods.csv\")\n#usda_food = pd.read_csv(r'C:\\Users\\Harshitha\\Desktop\\usda_sr_all_foods.csv')\nprint(usda_food[\"Food Group\"].unique())\nusda_food = usda_food.fillna(0)\nusda_food[\"Cereals\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Baked Foods\" else 1 if x==\"Breakfast Cereals\" else 1 if x==\"Grains and Pasta\" else 0.5 if x==\"Baby Foods\" else 0)\nusda_food[\"Fruits\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Fruits\" else 0 )\nusda_food[\"Vegetables\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Vegetables\" else 0 )\nusda_food[\"nuts\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Nuts and Seeds\" else 0.3 if x==\"Baby Foods\" else 0)\nusda_food[\"pulses\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Beans and Lentils\" else 0.3 if x==\"Baby Foods\" else 0 )\nusda_food[\"dairy\"] = usda_food[\"Food Group\"].apply(lambda x: 0.7 if x==\"Dairy and Egg Products\" else 0 )\nusda_food[\"non-veg\"] = usda_food[\"Food Group\"].apply(lambda x: 1 if x==\"Meats\" else 1 if x==\"Fish\" else 0 )\nusda_food[\"processd\"] = usda_food[\"Food Group\"].apply(lambda x: 1 if x==\"Beverages\" else 1 if x==\"Fast Foods\" else 1 if x==\"Soups and Sauces\" else 0 )\nusda_food.drop(columns=[\"Food Group\",\"name\",\"ID\",\"200 Calorie Weight (g)\",\"PRAL score\"], inplace= True)\nusda_cols = usda_food.columns.to_list()\nusda_cols = [\"Cereals\",\"Vegetables\",\"nuts\",\"pulses\",\"dairy\",\"non-veg\",\"processd\"]+usda_cols[:-8]\nprint(\"cholestol\"+str(usda_cols.index(\"Cholesterol (mg)\"))+\"  vita mcg\"+str(usda_cols.index(\"Vitamin A, RAE (mcg)\"))+\"  vit c\"+str(usda_cols.index(\"Vitamin C (mg)\")))\nusda_food = usda_food[usda_cols]\nprint(usda_food.head()); print(usda_food.shape);\nnorm = StandardScaler()\nusda_norm = norm.fit(usda_food)\nusda_norm = norm.transform(usda_food); \nprint(usda_norm.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tx045hPwnWVw","outputId":"8cf4f42f-b8df-4a0b-915a-48eb3b105304","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.max(usda_food.max()))\nusda_food.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#channels to scale\nusda_food_chn = np.empty((14164,100,2)) ; usda_norm_chn = np.empty((14164,100,2))\nusda_food_chn[:,:,0] = np.clip(usda_food,0,1000)\nusda_food_chn[:,:,1] = np.clip(usda_food,1000,100000)\n#usda_food_chn[:,:,2] = np.clip(usda_food,1000,1500)\n\nusda_food_chn = usda_food_chn[np.random.randint(0,14163,500),:,:]; usda_norm_chn = np.empty((500,100,2))\n\nchn_norm0 = MinMaxScaler()\nusda_norm_chn[:,:,0] = chn_norm0.fit_transform(usda_food_chn[:,:,0])\nchn_norm1 = MinMaxScaler()\nusda_norm_chn[:,:,1] = chn_norm1.fit_transform(usda_food_chn[:,:,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nkn = NearestNeighbors(n_neighbors=1).fit(usda_food)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6ixmjctnhnM","outputId":"fb7dbbe9-b69a-4d10-b5d1-b78f7f7134c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VAE architecture\n\nEncoder has a dense layer followed by 1D convolutional layers\n\nDecoder has sequence of AdaIN blocks of Transpose convolutional layers with instance normalization, sclaed and shifted by factor determined by Dense layer. The dense layer embeds feature information of USDA data distribution. AdaIN block aids in realising highly accurate results.","metadata":{"id":"bxwlNbsgpfz1"}},{"cell_type":"code","source":"import tensorflow_addons as tfa\n\nreg = tf.keras.regularizers.L2(0.0003)\nadn_reg = tf.keras.regularizers.L2(0.0001)\n\ndef adainblk(inp,w_nois,out_layers,reg):\n    x = tfa.layers.InstanceNormalization()(inp)\n    scl = layers.Dense(out_layers,activation='leaky_relu',kernel_regularizer=reg)(w_nois)\n    sft = layers.Dense(out_layers,activation='leaky_relu',kernel_regularizer=reg)(w_nois)\n    adn = tf.expand_dims(scl,axis=-2) + x * tf.expand_dims(sft,axis=-2)\n    return adn \n\n'''--***--auto encoder--***--'''\n#encoder\ninp = tf.keras.Input((100,2))\n#inp = tf.expand_dims(inp,axis=-1) \ninp = layers.BatchNormalization()(inp) \nx = layers.Conv1D(150,100,padding='same',activation='leaky_relu',kernel_regularizer=reg)(inp)\nx = layers.BatchNormalization()(x)\nx = layers.Conv1D(130,100,activation='leaky_relu',padding='same',kernel_regularizer=reg)(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv1D(90,5,activation='leaky_relu',padding='same', kernel_regularizer=reg)(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv1D(70,5,strides=2,activation='leaky_relu',padding='same',kernel_regularizer=reg)(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv1D(50,5,activation='leaky_relu',padding='same',kernel_regularizer=reg)(x)\nx = layers.BatchNormalization()(x)\nx1 = layers.Conv1D(30,5, activation='leaky_relu',padding='same',kernel_regularizer=reg)(x)\nemb_m = layers.Conv1D(1,5,activation='sigmoid',padding='same')(x1)\nemb_m = tf.squeeze(emb_m, axis=-1)\nx2 = layers.Conv1D(30,5, activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x) \nemb_v = layers.Conv1D(1,5,activation='sigmoid',padding='same')(x2) \nemb_v =tf.squeeze(emb_v,axis=-1) \nnois = tf.random.normal(tf.shape(emb_m)) \n\nemb = nois * tf.exp(emb_v * 0.5) + emb_m\n\nw_nois =  layers.Dense(15,activation='tanh')(layers.Flatten()(inp)) \n\n# decoder \n\ndec_inp = tf.expand_dims(emb,axis=-1) \n\nx = layers.Conv1DTranspose(100,5,strides=2,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(dec_inp)\nx = layers.Conv1DTranspose(80,5,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x) \nx = layers.Conv1DTranspose(70,5,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x)  \nx = layers.Conv1DTranspose(50,5,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x) \nx = layers.Conv1DTranspose(50,5,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x) \n\nx = layers.Conv1DTranspose(70,100,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg)(x)\nx = layers.Conv1DTranspose(70,100,activation=\"leaky_relu\",padding='same',kernel_regularizer=reg )(x)\nadn = adainblk(x,w_nois,70,adn_reg) ;\nx = layers.Conv1DTranspose(2,100,activation=\"leaky_relu\",padding='same' )(adn)\nadn = adainblk(x,w_nois,2,adn_reg) ;\n\n\nencdr = tf.keras.Model(inp,[emb_m,emb_v])\nencdec = tf.keras.Model(inp,adn) ; \nencdec.summary()\n\n\n\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZpBSiBFpfz1","outputId":"9c22cc62-e1e8-49de-fb3e-e7dc38c8358c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training VAE","metadata":{}},{"cell_type":"code","source":"\nclass intgrl_los(tf.keras.losses.Loss):\n    def call(self,y_true,y_pred):\n        dif = tfp.math.trapz(tf.math.abs(y_pred - y_true), axis=0)/250 ;\n        return dif  \n\nwloso = intgrl_los() \n\n\nclass vae(tf.keras.Model):\n    def __init__(self,mdl,real,los,opt):\n        super().__init__()\n        self.mdl = mdl;\n        self.rel = tf.dtypes.cast(real,tf.float32);\n        self.los=los; \n        self.opt= opt;      \n        \n    def compile(self,loss,optimizer,metrics): \n        super().compile() \n        self.mdl.compile(loss=self.los,optimizer=self.opt,metrics=[\"Accuracy\"]) \n    def train_step(self,dta):\n        dta = dta\n        for i in range(2):\n            with tf.GradientTape() as grdtp:\n                grdtp.watch(self.mdl.trainable_variables)\n                fke = self.mdl(self.rel); \n                lss = self.los(self.rel,fke) \n                cos_los = tf.keras.losses.cosine_similarity(self.rel,fke)\n                lss_tot = lss +1.5*(1+cos_los)**2 \n                \n                grd = grdtp.gradient(lss_tot,self.mdl.trainable_variables)\n            \n            self.opt.apply_gradients(zip(grd,self.mdl.trainable_variables))\n            \n        return {\"loss\":lss_tot,\" mae:\":2*tf.keras.losses.mean_absolute_error(self.rel,fke),'cos sim: ':cos_los}\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = tf.keras.optimizers.Adam(0.0005,beta_1=0.8,beta_2=0.88,epsilon=1e-5)\nwgno = vae(encdec,usda_norm_chn,wloso,opt)\nwgno.compile(loss=wgno.los,optimizer= opt,metrics=[\"Acccuracy\"]) \nwgno.fit(usda_norm,epochs=2) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training\nopt = tf.keras.optimizers.Adam(0.0005,beta_1=0.8,beta_2=0.88,epsilon=1e-5) \nwgno.compile(loss=wgno.los,optimizer= opt,metrics=[\"Acccuracy\"]) \nwgno.fit(usda_norm,epochs=10) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wgno.fit(usda_norm,epochs=20) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nReproduce the data","metadata":{"id":"-xuu8K8wjORt"}},{"cell_type":"code","source":"usda_chn = usda_food_chn[:,:,0]*1000 + usda_food_chn[:,:,1]+ usda_food_chn[:,:,2] *100000\n\nkn_ch = NearestNeighbors(n_neighbors=1).fit(usda_chn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prd = wgno.mdl.predict(usda_norm_chn[27:45,:,:]) \nprd = tf.clip_by_value(prd,0,1)\nfod = (prd[:,:,0]+ prd[:,:,1] ) * 1000\ndst,ind = kn.kneighbors(fod); \nprint(\"knn distance: \",dst, \"food index predicted: \",ind)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}